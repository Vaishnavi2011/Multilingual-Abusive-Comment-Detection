{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Kreva.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaishnavi2011/Multilingual-Abusive-Comment-Detection/blob/main/Kreva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-12-13T20:06:06.621616Z",
          "iopub.execute_input": "2021-12-13T20:06:06.622355Z",
          "iopub.status.idle": "2021-12-13T20:06:06.659377Z",
          "shell.execute_reply.started": "2021-12-13T20:06:06.62225Z",
          "shell.execute_reply": "2021-12-13T20:06:06.658278Z"
        },
        "trusted": true,
        "id": "ElecwJ91djXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def read_csv(path:str):\n",
        "    file = open(path, \"r\").read()\n",
        "    ix = []\n",
        "    ctx = []\n",
        "    la = []\n",
        "\n",
        "    for row in file.split(\"\\n\"):\n",
        "        l = re.sub(',(?!(?=[^\"]*\"[^\"]*(?:\"[^\"]*\"[^\"]*)*$))', \"\\t\", row)\n",
        "         \n",
        "        try:\n",
        "            lk = l.split(\"\\t\")\n",
        "            if len(lk)>2 and len(lk[0])<6:\n",
        "                p = lk[0]\n",
        "                q =  lk[1]\n",
        "                r = lk[len(lk)-1][-1:]\n",
        "                ix.append(p)\n",
        "                ctx.append(q)\n",
        "                la.append(r)\n",
        "                \n",
        "            else:\n",
        "                lk=row.replace('\"', \" \")\n",
        "                lk=lk.split(\",\")\n",
        "                p = lk[0]\n",
        "                q =  lk[1]\n",
        "                r = lk[len(lk)-1][-1:]\n",
        "                ix.append(p)\n",
        "                ctx.append(q)\n",
        "                la.append(r)\n",
        "        except Exception as e:\n",
        "            print(\"Exception occurred!.\", e)\n",
        "            print(f\"Length of ids obtained: {len(ix)}, and text: {len(ctx)}\")\n",
        "\n",
        "    dataset = pd.DataFrame()\n",
        "    dataset[\"CommentId\"]=ix[1:]\n",
        "    dataset[\"commentText\"]=ctx[1:]\n",
        "    dataset[\"label\"]=la[1:] \n",
        "    dataset = dataset.astype(dtype={\"CommentId\":int, \"commentText\":str, \"label\":int})\n",
        "    return dataset\n",
        "\n",
        "%time dataset = read_csv(\"/kaggle/input/multilingualabusivecomment/ShareChat-IndoML-Datathon-NSFW-CommentChallenge_Train.csv\")\n",
        "dataset.tail()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:42:51.925781Z",
          "iopub.execute_input": "2021-12-05T11:42:51.926752Z",
          "iopub.status.idle": "2021-12-05T11:43:11.447261Z",
          "shell.execute_reply.started": "2021-12-05T11:42:51.926687Z",
          "shell.execute_reply": "2021-12-05T11:43:11.446615Z"
        },
        "trusted": true,
        "id": "ZW--EMwNdjX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing"
      ],
      "metadata": {
        "id": "DWY0RoPsdjX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].apply(remove_emoji)\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('[^\\w\\s]','')\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('\\n','') #Replace nextLine\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('\\d+', '')\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('@', '') #Replaces mentions \n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('#', '')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:43:32.046406Z",
          "iopub.execute_input": "2021-12-05T11:43:32.046764Z",
          "iopub.status.idle": "2021-12-05T11:43:51.01262Z",
          "shell.execute_reply.started": "2021-12-05T11:43:32.046727Z",
          "shell.execute_reply": "2021-12-05T11:43:51.011747Z"
        },
        "trusted": true,
        "id": "6bEjxNxMdjX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "li = []\n",
        "for i in range(len(dataset[\"commentText\"])):\n",
        "    if(dataset[\"commentText\"][i] == ''):\n",
        "        li.append(i)\n",
        "dataset = dataset.drop(li)\n",
        "dataset = dataset.reset_index()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:43:51.014985Z",
          "iopub.execute_input": "2021-12-05T11:43:51.015569Z",
          "iopub.status.idle": "2021-12-05T11:44:03.071647Z",
          "shell.execute_reply.started": "2021-12-05T11:43:51.015516Z",
          "shell.execute_reply": "2021-12-05T11:44:03.070901Z"
        },
        "trusted": true,
        "id": "Wmspor2pdjX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:44:03.073278Z",
          "iopub.execute_input": "2021-12-05T11:44:03.074121Z",
          "iopub.status.idle": "2021-12-05T11:44:03.093887Z",
          "shell.execute_reply.started": "2021-12-05T11:44:03.07407Z",
          "shell.execute_reply": "2021-12-05T11:44:03.092849Z"
        },
        "trusted": true,
        "id": "zffJW5_wdjX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords list\n",
        "sw = ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', \"c'mon\", 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', \"couldn't\", 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', \"didn't\", 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', \"doesn't\", 'doing', 'done', 'dono', 'dont', \"don't\", 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'ha', 'had', 'hadd', 'hadn', 'hadnt', \"hadn't\", 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', 'havent', \"haven't\", 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', \"i'd\", 'idk', 'ie', 'if', \"i'll\", \"i'm\", 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', \"isn't\", 'iss', 'isse', 'issi', 'isski', 'it', \"it'd\", \"it'll\", 'itna', 'itne', 'itni', 'itno', 'its', \"it's\", 'itself', 'ityaadi', 'ityadi', \"i've\", 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kr', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kn', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lakin', 'lekin', 'less', 'lest', 'let', \"let's\", 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mei', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', \"mightn't\", 'mil', 'mujhy', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', \"mustn't\", 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', \"shan't\", 'she', \"she's\", 'should', 'shouldn', 'shouldnt', \"shouldn't\", \"should've\", 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', \"there's\", 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'TRUE', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'video', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', \"we'd\", 'well', \"we'll\", 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what's\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', \"won't\", 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yup']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:45:11.818328Z",
          "iopub.execute_input": "2021-12-05T11:45:11.81865Z",
          "iopub.status.idle": "2021-12-05T11:45:11.876834Z",
          "shell.execute_reply.started": "2021-12-05T11:45:11.818616Z",
          "shell.execute_reply": "2021-12-05T11:45:11.875246Z"
        },
        "trusted": true,
        "id": "v5Pv7qYJdjX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    lower_text = text.lower()\n",
        "    return word_tokenize(lower_text)\n",
        "\n",
        "\n",
        "def stem_and_lem(tokens):\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        token = wordnet_lemmatizer.lemmatize(token)\n",
        "        token = lancaster_stemmer.stem(token)\n",
        "        if len(token) > 1:\n",
        "            clean_tokens.append(token)\n",
        "    return clean_tokens\n",
        "\n",
        "def stopwords(tokens):\n",
        "    new_text = []\n",
        "    for word in tokens:\n",
        "        if word.lower() not in sw:\n",
        "            new_text.append(word) \n",
        "    return new_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:45:13.61246Z",
          "iopub.execute_input": "2021-12-05T11:45:13.613408Z",
          "iopub.status.idle": "2021-12-05T11:45:14.280349Z",
          "shell.execute_reply.started": "2021-12-05T11:45:13.613356Z",
          "shell.execute_reply": "2021-12-05T11:45:14.279403Z"
        },
        "trusted": true,
        "id": "ngE2ziWqdjX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Tokenizing Data...\")\n",
        "dataset['token'] = dataset['commentText'].progress_apply(tokenize)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:46:15.531906Z",
          "iopub.execute_input": "2021-12-05T11:46:15.532425Z",
          "iopub.status.idle": "2021-12-05T11:49:18.719564Z",
          "shell.execute_reply.started": "2021-12-05T11:46:15.532376Z",
          "shell.execute_reply": "2021-12-05T11:49:18.718721Z"
        },
        "trusted": true,
        "id": "Orssy9NOdjX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Removing Stopwords...\")\n",
        "dataset['token'] = dataset['token'].progress_apply(stopwords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:50:06.446078Z",
          "iopub.execute_input": "2021-12-05T11:50:06.447455Z",
          "iopub.status.idle": "2021-12-05T11:51:53.141792Z",
          "shell.execute_reply.started": "2021-12-05T11:50:06.447392Z",
          "shell.execute_reply": "2021-12-05T11:51:53.14083Z"
        },
        "trusted": true,
        "id": "imLMavZqdjX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Stemming And Lemmatizing\")\n",
        "dataset['token'] = dataset['token'].progress_apply(stem_and_lem)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:52:00.34098Z",
          "iopub.execute_input": "2021-12-05T11:52:00.341299Z",
          "iopub.status.idle": "2021-12-05T11:53:54.918737Z",
          "shell.execute_reply.started": "2021-12-05T11:52:00.341264Z",
          "shell.execute_reply": "2021-12-05T11:53:54.917751Z"
        },
        "trusted": true,
        "id": "8pPxT8LqdjX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:54:25.516007Z",
          "iopub.execute_input": "2021-12-05T11:54:25.51634Z",
          "iopub.status.idle": "2021-12-05T11:54:25.541487Z",
          "shell.execute_reply.started": "2021-12-05T11:54:25.516303Z",
          "shell.execute_reply": "2021-12-05T11:54:25.540079Z"
        },
        "trusted": true,
        "id": "rKNgkYTNdjX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization"
      ],
      "metadata": {
        "id": "Kgwi7I2PdjYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tfid(text_vector):\n",
        "    vectorizer = TfidfVectorizer(max_features=100,\n",
        "                                 min_df=5,\n",
        "                                 max_df=0.501)\n",
        "    untokenized_data =[' '.join(tweet) for tweet in tqdm(text_vector, \"Vectorizing...\")]\n",
        "    vectorizer = vectorizer.fit(untokenized_data)\n",
        "    vectors = vectorizer.transform(untokenized_data).toarray()\n",
        "    return vectors"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:54:41.975311Z",
          "iopub.execute_input": "2021-12-05T11:54:41.975834Z",
          "iopub.status.idle": "2021-12-05T11:54:41.982133Z",
          "shell.execute_reply.started": "2021-12-05T11:54:41.975793Z",
          "shell.execute_reply": "2021-12-05T11:54:41.981217Z"
        },
        "trusted": true,
        "id": "B4yPoViodjYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "text_vector = dataset['token'].tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:54:47.871312Z",
          "iopub.execute_input": "2021-12-05T11:54:47.87171Z",
          "iopub.status.idle": "2021-12-05T11:54:47.93976Z",
          "shell.execute_reply.started": "2021-12-05T11:54:47.871652Z",
          "shell.execute_reply": "2021-12-05T11:54:47.938615Z"
        },
        "trusted": true,
        "id": "deS6Y9XDdjYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature = tfid(text_vector)\n",
        "label = dataset['label']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:55:18.486376Z",
          "iopub.execute_input": "2021-12-05T11:55:18.486784Z",
          "iopub.status.idle": "2021-12-05T11:55:47.182809Z",
          "shell.execute_reply.started": "2021-12-05T11:55:18.486737Z",
          "shell.execute_reply": "2021-12-05T11:55:47.18208Z"
        },
        "trusted": true,
        "id": "eupSIQJedjYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "tb0Wiv9BdjYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "\n",
        "def classify(vectors, labels, type=\"DT\"):\n",
        "    # Random Splitting With Ratio 3 : 1\n",
        "    X_train, X_test, y_train, y_test = train_test_split(feature, label, test_size=0.25)\n",
        "\n",
        "    # Initialize Model\n",
        "    classifier = None\n",
        "    if(type==\"MNB\"):\n",
        "        print(\"Classification model - MultinomialNB\")\n",
        "        classifier = MultinomialNB(alpha=0.7)\n",
        "        classifier.fit(X_train, y_train)\n",
        "    elif(type==\"KNN\"):\n",
        "        print(\"Classification model - KNN\")\n",
        "        classifier = KNeighborsClassifier(n_jobs=4)\n",
        "        params = {'n_neighbors': [3,5,7,9], 'weights':['uniform', 'distance']}\n",
        "        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
        "        classifier.fit(X_train, y_train)\n",
        "        classifier = classifier.best_estimator_\n",
        "    elif(type==\"SVM\"):\n",
        "        print(\"Classification model - SVM\")\n",
        "        classifier = SVC()\n",
        "        classifier = GridSearchCV(classifier, {'C':[0.001, 0.01, 0.1, 1, 10]}, cv=3, n_jobs=4)\n",
        "        classifier.fit(X_train, y_train)\n",
        "        classifier = classifier.best_estimator_\n",
        "    elif(type==\"DT\"):\n",
        "        print(\"Classification model - Decision Tree Classifier\")\n",
        "        classifier = DecisionTreeClassifier(max_depth=800, min_samples_split=5)\n",
        "        params = {'criterion':['gini','entropy']}\n",
        "        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
        "        classifier.fit(X_train, y_train)\n",
        "        classifier = classifier.best_estimator_\n",
        "    elif(type==\"RF\"):\n",
        "        print(\"Classification model - Random Forest Classifier\")\n",
        "        classifier = RandomForestClassifier(max_depth=800, min_samples_split=5)\n",
        "        params = {'n_estimators': [n for n in range(50,200,50)], 'criterion':['gini','entropy'], }\n",
        "        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
        "        classifier.fit(X_train, y_train)\n",
        "        classifier = classifier.best_estimator_\n",
        "    elif(type==\"LR\"):\n",
        "        print(\"Classification model - Logistic Regression\")\n",
        "        classifier = LogisticRegression(multi_class='auto', solver='newton-cg',)\n",
        "        classifier = GridSearchCV(classifier, {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}, cv=3, n_jobs=4)\n",
        "        classifier.fit(X_train, y_train)\n",
        "        classifier = classifier.best_estimator_ \n",
        "\n",
        "    else:\n",
        "        print(\"Wrong Classifier Type!\")\n",
        "        return \n",
        "\n",
        "    accuracy = accuracy_score(y_train, classifier.predict(X_train))\n",
        "    print(\"Training Accuracy:\", accuracy)\n",
        "    y_predictions = classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test , y_predictions)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    print(\"Confusion Matrix:\", )\n",
        "    print(confusion_matrix(y_test , y_predictions))\n",
        "    print(classification_report(y_test , y_predictions))\n",
        "    joblib.dump(classifier, type)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:57:23.669432Z",
          "iopub.execute_input": "2021-12-05T11:57:23.669958Z",
          "iopub.status.idle": "2021-12-05T11:57:23.819498Z",
          "shell.execute_reply.started": "2021-12-05T11:57:23.66992Z",
          "shell.execute_reply": "2021-12-05T11:57:23.818233Z"
        },
        "trusted": true,
        "id": "pB40GJx_djYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify(feature, label, \"RF\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T11:57:30.470711Z",
          "iopub.execute_input": "2021-12-05T11:57:30.471006Z",
          "iopub.status.idle": "2021-12-05T12:01:25.749623Z",
          "shell.execute_reply.started": "2021-12-05T11:57:30.470971Z",
          "shell.execute_reply": "2021-12-05T12:01:25.748588Z"
        },
        "trusted": true,
        "id": "b9Y_4A6cdjYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Data"
      ],
      "metadata": {
        "id": "WwJ3531_djYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def read_csv(path:str):\n",
        "    file = open(path, \"r\").read()\n",
        "    ix = []\n",
        "    ctx = []\n",
        "\n",
        "    for row in file.split(\"\\n\"):\n",
        "        l = re.sub(',(?!(?=[^\"]*\"[^\"]*(?:\"[^\"]*\"[^\"]*)*$))', \"\\t\", row)\n",
        "        try:\n",
        "            lk = l.split(\"\\t\")\n",
        "            if len(lk)>2 and len(lk[0])<6:\n",
        "                p,q= lk[0], lk[1]\n",
        "                ix.append(p)\n",
        "                ctx.append(q)\n",
        "            else:\n",
        "                lk=row.replace('\"', \" \")\n",
        "                lk=lk.split(\",\")\n",
        "                p,q = lk[0], lk[1]\n",
        "                ix.append(p)\n",
        "                ctx.append(q)\n",
        "        except Exception as e:\n",
        "            print(\"Exception occurred!.\", e)\n",
        "            print(f\"Length of ids obtained: {len(ix)}, and text: {len(ctx)}\")\n",
        "\n",
        "    dataset = pd.DataFrame()\n",
        "    dataset[\"CommentId\"]=ix[1:]\n",
        "    dataset[\"commentText\"]=ctx[1:]\n",
        "    dataset = dataset.astype(dtype={\"CommentId\":int, \"commentText\":str})\n",
        "    return dataset\n",
        "\n",
        "%time dataset = read_csv(\"../input/multilingualabusivecomment/ShareChat-IndoML-Datathon-NSFW-CommentChallenge_Test_NoLabel.csv\")\n",
        "dataset.tail()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:28:26.886297Z",
          "iopub.execute_input": "2021-12-15T09:28:26.886698Z",
          "iopub.status.idle": "2021-12-15T09:28:34.904511Z",
          "shell.execute_reply.started": "2021-12-15T09:28:26.886601Z",
          "shell.execute_reply": "2021-12-15T09:28:34.903554Z"
        },
        "trusted": true,
        "id": "0VEo2HSedjYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:28:34.906337Z",
          "iopub.execute_input": "2021-12-15T09:28:34.906587Z",
          "iopub.status.idle": "2021-12-15T09:28:34.983207Z",
          "shell.execute_reply.started": "2021-12-15T09:28:34.906556Z",
          "shell.execute_reply": "2021-12-15T09:28:34.982269Z"
        },
        "trusted": true,
        "id": "CjkhE1aadjYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].apply(remove_emoji)\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('[^\\w\\s]','')\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('\\n','') #Replace nextLine\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('\\d+', '')\n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('@', '') #Replaces mentions \n",
        "dataset[\"commentText\"] = dataset[\"commentText\"].str.replace('#', '')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:28:51.200524Z",
          "iopub.execute_input": "2021-12-15T09:28:51.200821Z",
          "iopub.status.idle": "2021-12-15T09:28:58.372442Z",
          "shell.execute_reply.started": "2021-12-15T09:28:51.200787Z",
          "shell.execute_reply": "2021-12-15T09:28:58.371586Z"
        },
        "trusted": true,
        "id": "RFPgvfR9djYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', \"c'mon\", 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', \"couldn't\", 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', \"didn't\", 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', \"doesn't\", 'doing', 'done', 'dono', 'dont', \"don't\", 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'ha', 'had', 'hadd', 'hadn', 'hadnt', \"hadn't\", 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', 'havent', \"haven't\", 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', \"i'd\", 'idk', 'ie', 'if', \"i'll\", \"i'm\", 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', \"isn't\", 'iss', 'isse', 'issi', 'isski', 'it', \"it'd\", \"it'll\", 'itna', 'itne', 'itni', 'itno', 'its', \"it's\", 'itself', 'ityaadi', 'ityadi', \"i've\", 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kr', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kn', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lakin', 'lekin', 'less', 'lest', 'let', \"let's\", 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mei', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', \"mightn't\", 'mil', 'mujhy', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', \"mustn't\", 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', \"shan't\", 'she', \"she's\", 'should', 'shouldn', 'shouldnt', \"shouldn't\", \"should've\", 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', \"there's\", 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'TRUE', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'video', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', \"we'd\", 'well', \"we'll\", 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what's\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', \"won't\", 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yup']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:29:07.434423Z",
          "iopub.execute_input": "2021-12-15T09:29:07.434811Z",
          "iopub.status.idle": "2021-12-15T09:29:07.591672Z",
          "shell.execute_reply.started": "2021-12-15T09:29:07.434763Z",
          "shell.execute_reply": "2021-12-15T09:29:07.590697Z"
        },
        "trusted": true,
        "id": "izDBD8bXdjYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(tweet):\n",
        "    lower_tweet = tweet.lower()\n",
        "    return word_tokenize(lower_tweet)\n",
        "\n",
        "\n",
        "def stem_and_lem(tokens):\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        token = wordnet_lemmatizer.lemmatize(token)\n",
        "        token = lancaster_stemmer.stem(token)\n",
        "        if len(token) > 1:\n",
        "            clean_tokens.append(token)\n",
        "    return clean_tokens\n",
        "\n",
        "def stopwords(tokens):\n",
        "    new_text = []\n",
        "    for word in tokens:\n",
        "        if word.lower() not in sw:\n",
        "            new_text.append(word) \n",
        "    return new_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:29:17.010600Z",
          "iopub.execute_input": "2021-12-15T09:29:17.011720Z",
          "iopub.status.idle": "2021-12-15T09:29:17.020386Z",
          "shell.execute_reply.started": "2021-12-15T09:29:17.011667Z",
          "shell.execute_reply": "2021-12-15T09:29:17.019709Z"
        },
        "trusted": true,
        "id": "lWYz_4dBdjYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Tokenizing Data...\")\n",
        "dataset['token'] = dataset['commentText'].progress_apply(tokenize)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:29:20.212221Z",
          "iopub.execute_input": "2021-12-15T09:29:20.212733Z",
          "iopub.status.idle": "2021-12-15T09:30:39.905545Z",
          "shell.execute_reply.started": "2021-12-15T09:29:20.212671Z",
          "shell.execute_reply": "2021-12-15T09:30:39.904587Z"
        },
        "trusted": true,
        "id": "w0vw6qEndjYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Removing Stopwords...\")\n",
        "dataset['token'] = dataset['token'].progress_apply(stopwords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:30:47.744279Z",
          "iopub.execute_input": "2021-12-15T09:30:47.744561Z",
          "iopub.status.idle": "2021-12-15T09:31:33.752601Z",
          "shell.execute_reply.started": "2021-12-15T09:30:47.744530Z",
          "shell.execute_reply": "2021-12-15T09:31:33.751693Z"
        },
        "trusted": true,
        "id": "jSNhTaIhdjYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Stemming And Lemmatizing\")\n",
        "dataset['token'] = dataset['token'].progress_apply(stem_and_lem)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:31:33.754015Z",
          "iopub.execute_input": "2021-12-15T09:31:33.754332Z",
          "iopub.status.idle": "2021-12-15T09:32:22.232455Z",
          "shell.execute_reply.started": "2021-12-15T09:31:33.754302Z",
          "shell.execute_reply": "2021-12-15T09:32:22.231776Z"
        },
        "trusted": true,
        "id": "KIVC5bRPdjYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tfid(text_vector):\n",
        "    vectorizer = TfidfVectorizer(max_features=100)\n",
        "    untokenized_data =[' '.join(tweet) for tweet in tqdm(text_vector, \"Vectorizing...\")]\n",
        "    vectorizer = vectorizer.fit(untokenized_data)\n",
        "    vectors = vectorizer.transform(untokenized_data).toarray()\n",
        "    return vectors"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:32:22.233623Z",
          "iopub.execute_input": "2021-12-15T09:32:22.233818Z",
          "iopub.status.idle": "2021-12-15T09:32:22.239952Z",
          "shell.execute_reply.started": "2021-12-15T09:32:22.233793Z",
          "shell.execute_reply": "2021-12-15T09:32:22.239033Z"
        },
        "trusted": true,
        "id": "t1gL9TdudjYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "text_vector = dataset['token'].tolist()\n",
        "feature = tfid(text_vector)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:32:22.241928Z",
          "iopub.execute_input": "2021-12-15T09:32:22.242460Z",
          "iopub.status.idle": "2021-12-15T09:32:34.984593Z",
          "shell.execute_reply.started": "2021-12-15T09:32:22.242429Z",
          "shell.execute_reply": "2021-12-15T09:32:34.983615Z"
        },
        "trusted": true,
        "id": "Y16ywA9kdjYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "#./RF\n",
        "clf = joblib.load('./RF')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:32:34.986283Z",
          "iopub.execute_input": "2021-12-15T09:32:34.986518Z",
          "iopub.status.idle": "2021-12-15T09:32:37.476145Z",
          "shell.execute_reply.started": "2021-12-15T09:32:34.986490Z",
          "shell.execute_reply": "2021-12-15T09:32:37.475272Z"
        },
        "trusted": true,
        "id": "WX-GVNjedjYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = clf.predict(feature)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:33:02.098035Z",
          "iopub.execute_input": "2021-12-15T09:33:02.098368Z",
          "iopub.status.idle": "2021-12-15T09:33:58.611423Z",
          "shell.execute_reply.started": "2021-12-15T09:33:02.098333Z",
          "shell.execute_reply": "2021-12-15T09:33:58.610370Z"
        },
        "trusted": true,
        "id": "tp9rcQWZdjYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['Label'] = list(y_predict)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:33:58.613250Z",
          "iopub.execute_input": "2021-12-15T09:33:58.613504Z",
          "iopub.status.idle": "2021-12-15T09:33:59.160170Z",
          "shell.execute_reply.started": "2021-12-15T09:33:58.613473Z",
          "shell.execute_reply": "2021-12-15T09:33:59.159433Z"
        },
        "trusted": true,
        "id": "AH8fy4XRdjYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:33:59.161423Z",
          "iopub.execute_input": "2021-12-15T09:33:59.161798Z",
          "iopub.status.idle": "2021-12-15T09:33:59.179734Z",
          "shell.execute_reply.started": "2021-12-15T09:33:59.161752Z",
          "shell.execute_reply": "2021-12-15T09:33:59.179142Z"
        },
        "trusted": true,
        "id": "GDzPy7KqdjYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[['CommentId','Label']].to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:36:58.605300Z",
          "iopub.execute_input": "2021-12-15T09:36:58.605570Z",
          "iopub.status.idle": "2021-12-15T09:36:59.810169Z",
          "shell.execute_reply.started": "2021-12-15T09:36:58.605541Z",
          "shell.execute_reply": "2021-12-15T09:36:59.809400Z"
        },
        "trusted": true,
        "id": "n8hMUcAWdjYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('./preprocessed_dataset.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-15T09:37:06.861732Z",
          "iopub.execute_input": "2021-12-15T09:37:06.862015Z",
          "iopub.status.idle": "2021-12-15T09:37:06.977826Z",
          "shell.execute_reply.started": "2021-12-15T09:37:06.861985Z",
          "shell.execute_reply": "2021-12-15T09:37:06.976945Z"
        },
        "trusted": true,
        "id": "kmIq6OJQdjYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6S8oiAuVdjYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}